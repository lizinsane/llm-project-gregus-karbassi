{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bad2eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "markdown_path is: docs/extracted_content_geschichtsbuch.md\n",
      "json_chunk_path is: docs/chunks_geschichtsbuch.json\n"
     ]
    }
   ],
   "source": [
    "# Pfad zu Markdown File festlegen\n",
    "\n",
    "#extract = \"origin\"\n",
    "extract = \"geschichtsbuch\"\n",
    "\n",
    "markdown_path_origin = \"docs/extracted_content_dan_brown.md\"\n",
    "markdown_path_path_geschichtsbuch = \"docs/extracted_content_geschichtsbuch.md\" \n",
    "\n",
    "json_chunk_origin = \"docs/chunks_dan_brown.json\"\n",
    "json_chunk_geschichtsbuch = \"docs/chunks_geschichtsbuch.json\"\n",
    "\n",
    "if extract == \"geschichtsbuch\": \n",
    "    markdown_path = markdown_path_path_geschichtsbuch\n",
    "    json_chunk_path = json_chunk_geschichtsbuch\n",
    "else:\n",
    "    markdown_path = markdown_path_origin\n",
    "    json_chunk_path = json_chunk_origin\n",
    "\n",
    "print(f\"markdown_path is: {markdown_path}\")\n",
    "print(f\"json_chunk_path is: {json_chunk_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca428a1b",
   "metadata": {},
   "source": [
    "## Parameter für Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d1620af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking-Parameter\n",
    "CHUNK_SIZE = 1500\n",
    "OVERLAP = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7370b8da",
   "metadata": {},
   "source": [
    "## Markdown-Datei einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "259bb40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datei geladen: docs/extracted_content_geschichtsbuch.md\n",
      "Gesamtlänge: 479526 Zeichen\n",
      "Gesamtlänge: 74594 Wörter\n"
     ]
    }
   ],
   "source": [
    "# Markdown-Datei einlesen\n",
    "with open(markdown_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    markdown_text = f.read()\n",
    "\n",
    "print(f\"Datei geladen: {markdown_path}\")\n",
    "print(f\"Gesamtlänge: {len(markdown_text)} Zeichen\")\n",
    "print(f\"Gesamtlänge: {len(markdown_text.split())} Wörter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f839d9",
   "metadata": {},
   "source": [
    "## Character Splitting implementieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc9bbbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Text wurde in 369 Chunks aufgeteilt\n",
      "============================================================\n",
      "\n",
      "Erste 3 Chunks (Vorschau):\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Chunk 1 (Länge: 1500 Zeichen, Seite(n): [12]) ---\n",
      "\n",
      "\n",
      "## Seite 12\n",
      "\n",
      "Vorwort\n",
      "Vorliegendes Buch baut auf die 'Schweizer Geschichte \" von Dr. Ludwig Suter auf, die sich bei der obligatorischen Einführung in den Sekundär-, Bezirks -, Fortbildungs-, Realschu...\n",
      "\n",
      "--- Chunk 2 (Länge: 1500 Zeichen, Seite(n): [12, 13]) ---\n",
      " Sekundarschüler vorher meist keinen Unterricht in der allgemeinen Geschichte genossen haben und darum einer kurzen Einführung in die Vorgeschichte unseres Landes bedürfen , wodurch sie das Werden und...\n",
      "\n",
      "--- Chunk 3 (Länge: 1500 Zeichen, Seite(n): [13]) ---\n",
      "hen Darstellungen gewähren auch einen Rückblick auf die religiösen Verhältnisse nach der Reformation und auf die Zustände der gleichen Periode.\n",
      "Auch in methodischer Hinsicht habe ich mich bestrebt , d...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_page_info(text, chunk_start, chunk_end):\n",
    "    \"\"\"\n",
    "    Extrahiert Seiteninformationen für einen gegebenen Text-Bereich.\n",
    "    Sucht nach '## Seite X' Markern im Text.\n",
    "    \"\"\"\n",
    "    # Finde alle Seiten-Marker im gesamten Text bis zum Chunk-Ende\n",
    "    page_pattern = r'## Seite (\\d+)'\n",
    "    pages_found = []\n",
    "    \n",
    "    # Finde alle Seiten-Marker mit ihren Positionen\n",
    "    for match in re.finditer(page_pattern, text[:chunk_end]):\n",
    "        page_num = int(match.group(1))\n",
    "        page_pos = match.start()\n",
    "        pages_found.append((page_num, page_pos))\n",
    "    \n",
    "    if not pages_found:\n",
    "        return None\n",
    "    \n",
    "    # Finde die Seite(n), auf der/denen sich der Chunk befindet\n",
    "    chunk_pages = set()\n",
    "    \n",
    "    # Seite am Anfang des Chunks\n",
    "    for page_num, page_pos in reversed(pages_found):\n",
    "        if page_pos <= chunk_start:\n",
    "            chunk_pages.add(page_num)\n",
    "            break\n",
    "    \n",
    "    # Alle Seiten innerhalb des Chunks\n",
    "    for page_num, page_pos in pages_found:\n",
    "        if chunk_start <= page_pos < chunk_end:\n",
    "            chunk_pages.add(page_num)\n",
    "    \n",
    "    return sorted(list(chunk_pages)) if chunk_pages else None\n",
    "\n",
    "def character_split(text, chunk_size, overlap):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    \n",
    "    while start < text_length:\n",
    "        \n",
    "        end = start + chunk_size # Ende des Chunks\n",
    "        \n",
    "        chunk = text[start:end]\n",
    "        page_info = extract_page_info(text, start, end)\n",
    "        \n",
    "        chunks.append({\n",
    "            'text': chunk,\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'pages': page_info\n",
    "        })\n",
    "        \n",
    "        start = end - overlap # Nächste Startposition: Ende minus Overlap\n",
    "        \n",
    "        # Verhindere Endlosschleife bei sehr kleinen Texten\n",
    "        if start + chunk_size >= text_length and start < text_length:\n",
    "            # Letzter Chunk\n",
    "            chunk = text[start:]\n",
    "            page_info = extract_page_info(text, start, text_length)\n",
    "            chunks.append({\n",
    "                'text': chunk,\n",
    "                'start': start,\n",
    "                'end': text_length,\n",
    "                'pages': page_info\n",
    "            })\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Text in Chunks aufteilen\n",
    "chunks = character_split(markdown_text, CHUNK_SIZE, OVERLAP)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Text wurde in {len(chunks)} Chunks aufgeteilt\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nErste 3 Chunks (Vorschau):\")\n",
    "print(f\"-\"*60)\n",
    "\n",
    "for i, chunk_data in enumerate(chunks[:3]):\n",
    "    chunk_text = chunk_data['text']\n",
    "    pages = chunk_data['pages']\n",
    "    print(f\"\\n--- Chunk {i+1} (Länge: {len(chunk_text)} Zeichen, Seite(n): {pages}) ---\")\n",
    "    print(chunk_text[:200] + \"...\" if len(chunk_text) > 200 else chunk_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cacd88",
   "metadata": {},
   "source": [
    "## Statistiken der Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d53ea19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Chunks: 369\n",
      "Durchschnittliche Chunk-Länge: 1499 Zeichen\n",
      "Kürzester Chunk: 1126 Zeichen\n",
      "Längster Chunk: 1500 Zeichen\n",
      "\n",
      "Seiten mit Inhalt: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 248, 251, 254, 257, 260, 263, 266]\n",
      "Anzahl Seiten: 240\n",
      "\n",
      "============================================================\n",
      "Token-Check für ersten Chunk:\n",
      "============================================================\n",
      "Zeichen: 1500\n",
      "Tokens: 371\n",
      "Status: ✅ OK (unter 512)\n"
     ]
    }
   ],
   "source": [
    "# Statistiken über die Chunks\n",
    "chunk_lengths = [len(chunk['text']) for chunk in chunks]\n",
    "\n",
    "print(f\"Anzahl Chunks: {len(chunks)}\")\n",
    "print(f\"Durchschnittliche Chunk-Länge: {sum(chunk_lengths) / len(chunk_lengths):.0f} Zeichen\")\n",
    "print(f\"Kürzester Chunk: {min(chunk_lengths)} Zeichen\")\n",
    "print(f\"Längster Chunk: {max(chunk_lengths)} Zeichen\")\n",
    "\n",
    "# Zeige Seiteninformationen\n",
    "pages_with_content = set()\n",
    "for chunk in chunks:\n",
    "    if chunk['pages']:\n",
    "        pages_with_content.update(chunk['pages'])\n",
    "\n",
    "print(f\"\\nSeiten mit Inhalt: {sorted(pages_with_content)}\")\n",
    "print(f\"Anzahl Seiten: {len(pages_with_content)}\")\n",
    "\n",
    "# Überprüfe mit Tokenizer (falls geladen)\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
    "    \n",
    "    # Teste ersten Chunk mit Prefix\n",
    "    sample_chunk = f\"passage: {chunks[0]['text']}\"\n",
    "    tokens = tokenizer(sample_chunk, return_tensors='pt', truncation=False)\n",
    "    num_tokens = tokens['input_ids'].shape[1]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Token-Check für ersten Chunk:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Zeichen: {len(chunks[0]['text'])}\")\n",
    "    print(f\"Tokens: {num_tokens}\")\n",
    "    print(f\"Status: {'✅ OK (unter 512)' if num_tokens <= 512 else '❌ ZU LANG'}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nTokenizer nicht verfügbar: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b4f73",
   "metadata": {},
   "source": [
    "## Chunks speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d06d48e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 369 Chunks gespeichert in: docs/chunks_geschichtsbuch.json\n",
      "Dateigröße: 615.8 KB\n",
      "\n",
      "Beispiel Chunk-Metadaten:\n",
      "{\n",
      "  \"chunk_id\": 0,\n",
      "  \"text\": \"\\n\\n## Seite 12\\n\\nVorwort\\nVorliegendes Buch baut auf die 'Schweizer Geschichte \\\" von Dr. Ludwig Suter auf, die sich bei der obligatorischen Einführung in den Sekundär-, Bezirks -, Fortbildungs-, Realschulen , Gymnasien und Lehrerseminare vor züglich bewährt hat.\\nIm ...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Dateiname basierend auf dem Quell-Dokument\n",
    "output_file = json_chunk_path\n",
    "\n",
    "# Chunks mit Metadaten speichern\n",
    "chunks_data = {\n",
    "    \"source_file\": markdown_path,\n",
    "    \"chunk_size\": CHUNK_SIZE,\n",
    "    \"overlap\": OVERLAP,\n",
    "    \"total_chunks\": len(chunks),\n",
    "    \"chunks\": [\n",
    "        {\n",
    "            \"chunk_id\": i,\n",
    "            \"text\": chunk['text'],\n",
    "            \"length\": len(chunk['text']),\n",
    "            \"pages\": chunk['pages'],\n",
    "            \"start_pos\": chunk['start'],\n",
    "            \"end_pos\": chunk['end']\n",
    "        }\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Als JSON speichern\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ {len(chunks)} Chunks gespeichert in: {output_file}\")\n",
    "print(f\"Dateigröße: {Path(output_file).stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"\\nBeispiel Chunk-Metadaten:\")\n",
    "print(json.dumps(chunks_data['chunks'][0], ensure_ascii=False, indent=2)[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f504f31",
   "metadata": {},
   "source": [
    "## Chunks wieder einlesen (zum Testen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddeffbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geladene Daten:\n",
      "  Quelle: docs/extracted_content_geschichtsbuch.md\n",
      "  Chunk-Größe: 1500\n",
      "  Overlap: 200\n",
      "  Anzahl Chunks: 369\n",
      "\n",
      "✅ 369 Chunks erfolgreich geladen\n",
      "\n",
      "Beispiel - Erster Chunk (erste 150 Zeichen):\n",
      "\n",
      "\n",
      "## Seite 12\n",
      "\n",
      "Vorwort\n",
      "Vorliegendes Buch baut auf die 'Schweizer Geschichte \" von Dr. Ludwig Suter auf, die sich bei der obligatorischen Einführung in...\n",
      "\n",
      "Seiteninformationen für erste 3 Chunks:\n",
      "  Chunk 1: Seite(n) [12]\n",
      "  Chunk 2: Seite(n) [12, 13]\n",
      "  Chunk 3: Seite(n) [13]\n"
     ]
    }
   ],
   "source": [
    "# Chunks wieder einlesen\n",
    "with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded_data = json.load(f)\n",
    "\n",
    "# Metadaten anzeigen\n",
    "print(\"Geladene Daten:\")\n",
    "print(f\"  Quelle: {loaded_data['source_file']}\")\n",
    "print(f\"  Chunk-Größe: {loaded_data['chunk_size']}\")\n",
    "print(f\"  Overlap: {loaded_data['overlap']}\")\n",
    "print(f\"  Anzahl Chunks: {loaded_data['total_chunks']}\")\n",
    "\n",
    "# Die eigentlichen Text-Chunks extrahieren\n",
    "loaded_chunks = [chunk_data[\"text\"] for chunk_data in loaded_data[\"chunks\"]]\n",
    "\n",
    "print(f\"\\n✅ {len(loaded_chunks)} Chunks erfolgreich geladen\")\n",
    "print(f\"\\nBeispiel - Erster Chunk (erste 150 Zeichen):\")\n",
    "print(loaded_chunks[0][:150] + \"...\")\n",
    "print(f\"\\nSeiteninformationen für erste 3 Chunks:\")\n",
    "for i in range(min(3, len(loaded_data['chunks']))):\n",
    "    chunk_info = loaded_data['chunks'][i]\n",
    "    print(f\"  Chunk {i+1}: Seite(n) {chunk_info['pages']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
