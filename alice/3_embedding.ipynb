{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05df1779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_chunk_path is: docs/chunks_geschichtsbuch.json\n",
      "json_embedding_path is: docs/embeddings_geschichtsbuch.json\n",
      "chroma_collection is: geschichtsbuch_collection\n",
      "chroma_collection_db is: docs/geschichtsbuch_chroma_db\n"
     ]
    }
   ],
   "source": [
    "# Pfad zu Chunk File festlegen\n",
    "\n",
    "#extract = \"origin\"\n",
    "extract = \"geschichtsbuch\"\n",
    "\n",
    "\n",
    "json_chunk_origin = \"docs/chunks_dan_brown.json\"\n",
    "json_chunk_geschichtsbuch = \"docs/chunks_geschichtsbuch.json\"\n",
    "\n",
    "json_embedding_geschichtsbuch = \"docs/embeddings_geschichtsbuch.json\"\n",
    "json_embedding_origin = \"docs/embeddings_dan_brown.json\"\n",
    "\n",
    "chroma_collection_geschichtsbuch = \"geschichtsbuch_collection\"\n",
    "chroma_collection_origin = \"dan_brown_collection\"\n",
    "\n",
    "chroma_db_geschichtsbuch = \"docs/geschichtsbuch_chroma_db\"\n",
    "chroma_db_origin = \"docs/dan_brown_chroma_db\"\n",
    "\n",
    "\n",
    "if extract == \"geschichtsbuch\": \n",
    "    json_chunk_path = json_chunk_geschichtsbuch\n",
    "    json_embedding_path = json_embedding_geschichtsbuch\n",
    "    chroma_collection = chroma_collection_geschichtsbuch\n",
    "    chroma_collection_db = chroma_db_geschichtsbuch\n",
    "else:\n",
    "    json_chunk_path = json_chunk_origin\n",
    "    json_embedding_path = json_embedding_origin\n",
    "    chroma_collection = chroma_collection_origin\n",
    "    chroma_collection_db = chroma_db_origin\n",
    "\n",
    "print(f\"json_chunk_path is: {json_chunk_path}\")\n",
    "print(f\"json_embedding_path is: {json_embedding_path}\")\n",
    "print(f\"chroma_collection is: {chroma_collection}\")\n",
    "print(f\"chroma_collection_db is: {chroma_collection_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35260b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benötigte Bibliotheken importieren\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48c1ae87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl geladener Chunks: 369\n",
      "Beispiel-Chunk: {'chunk_id': 0, 'text': '\\n\\n## Seite 12\\n\\nVorwort\\nVorliegendes Buch baut auf die \\'Schweizer Geschichte \" von Dr. Ludwig Suter auf, die sich bei der obligatorischen Einführung in den Sekundär-, Bezirks -, Fortbildungs-, Realschulen , Gymnasien und Lehrerseminare vor züglich bewährt hat.\\nIm Verlaufe der Schuljahre haben aber Fachund Kennerkreise , ge stützt auf ihre praktischen Erfahrungen, doch den Wunsch geäußert, es möchte manchenorts namentlich für die untern Stufen der Sekundär - , Fort bildungs -und Mittelschulen auf der Grundlage der \\'Schweizer Geschichte \" von Or. Ludwig Suter ein Lehrmittel geschaffen werden, das inhaltlich und methodisch auf die noch weniger entwickelte Fassungs kraft der Schüler dieser Stufe speziell Rücksicht nimmt. Dem Rate des Autors folgend , betraute die VerlagsanstaltBenziger L Co.A. G.in Einsiedeln mich mit dieser Aufgabe . Ob und wie weit sie gelungen ist , mögen die HH. Lehrer und Fachprofessoren , die auf dieser Stufe unterrichten , beurteilen.\\nBei meiner Arbeit waren mir also zwei Hauptgedanken wegleitend. Zunächst versuchte ich den geschichtlichen Lehrstoff wesentlich zu kürzen. Namentlichin den drei ersten Zeiträumen trat eine erhebliche Reduktion ein. Immerhin konnte auf die Periode vor der Gründung der Eid genossenschaft nicht gänzlich verzichtet werden, weil unsere Sekundarschüler vorher meist keinen Unterricht in der allgemeinen Geschichte genossen haben und darum einer kurzen Einführung in die Vorgeschichte unseres Landes bedürfen , wodurch sie das Werden und', 'length': 1500, 'pages': [12], 'start_pos': 0, 'end_pos': 1500}\n"
     ]
    }
   ],
   "source": [
    "# Chunks aus JSON-Datei laden\n",
    "with open(json_chunk_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    chunks = data['chunks']\n",
    "\n",
    "print(f\"Anzahl geladener Chunks: {len(chunks)}\")\n",
    "print(f\"Beispiel-Chunk: {chunks[0] if chunks else 'Keine Chunks gefunden'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26651174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell intfloat/multilingual-e5-small erfolgreich geladen\n"
     ]
    }
   ],
   "source": [
    "# Modell und Tokenizer laden\n",
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Modell in Evaluation-Modus setzen\n",
    "model.eval()\n",
    "\n",
    "print(f\"Modell {model_name} erfolgreich geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f98e8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding-Funktion definiert\n"
     ]
    }
   ],
   "source": [
    "# Funktion zum Erstellen von Embeddings\n",
    "def get_embeddings(texts, batch_size=32):\n",
    "    \"\"\"\n",
    "    Erstellt Embeddings für eine Liste von Texten\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        # Für E5-Modelle: Text mit \"query: \" oder \"passage: \" prefix versehen\n",
    "        # Hier verwenden wir \"passage: \" für Dokument-Chunks\n",
    "        batch_with_prefix = [f\"passage: {text}\" for text in batch]\n",
    "        \n",
    "        # Tokenisierung\n",
    "        encoded = tokenizer(\n",
    "            batch_with_prefix,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Embeddings erstellen (ohne Gradient-Berechnung)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            # Mean pooling über alle Token (außer Padding)\n",
    "            attention_mask = encoded['attention_mask']\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            \n",
    "            # Masked mean pooling\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            \n",
    "            # Normalisierung\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "        \n",
    "        if (i + batch_size) % 100 == 0:\n",
    "            print(f\"Verarbeitet: {min(i + batch_size, len(texts))}/{len(texts)} Chunks\")\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"Embedding-Funktion definiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cecce5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Embedding-Erstellung...\n",
      "\n",
      "Embeddings erstellt!\n",
      "Shape: (369, 384)\n",
      "Embedding-Dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Embeddings für alle Chunks erstellen\n",
    "print(\"Starte Embedding-Erstellung...\")\n",
    "\n",
    "# Je nach Chunk-Format anpassen (entweder Liste von Strings oder Liste von Dicts)\n",
    "if isinstance(chunks[0], dict):\n",
    "    # Wenn Chunks Dictionaries sind, Text-Feld extrahieren\n",
    "    chunk_texts = [chunk.get('text', chunk.get('content', str(chunk))) for chunk in chunks]\n",
    "else:\n",
    "    # Wenn Chunks bereits Strings sind\n",
    "    chunk_texts = chunks\n",
    "\n",
    "embeddings = get_embeddings(chunk_texts)\n",
    "\n",
    "print(f\"\\nEmbeddings erstellt!\")\n",
    "print(f\"Shape: {embeddings.shape}\")\n",
    "print(f\"Embedding-Dimension: {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1fe1c4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beispiel (mit Seiteninformation): Chunk 0 -> Seite(n): [12]\n",
      "Anzahl gespeicherter Embeddings: 369\n",
      "Embeddings gespeichert in: docs/embeddings_geschichtsbuch.json\n"
     ]
    }
   ],
   "source": [
    "# Embeddings speichern\n",
    "output_data = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if isinstance(chunk, dict):\n",
    "        # Wenn Chunk bereits ein Dictionary ist, Embedding hinzufügen\n",
    "        chunk_data = chunk.copy()\n",
    "        chunk_data['embedding'] = embeddings[i].tolist()\n",
    "        # Stelle sicher, dass Seiteninformationen enthalten sind\n",
    "        if 'pages' not in chunk_data:\n",
    "            chunk_data['pages'] = None\n",
    "    else:\n",
    "        # Wenn Chunk ein String ist, Dictionary erstellen\n",
    "        chunk_data = {\n",
    "            'text': chunk,\n",
    "            'embedding': embeddings[i].tolist(),\n",
    "            'pages': None\n",
    "        }\n",
    "    output_data.append(chunk_data)\n",
    "\n",
    "# In JSON-Datei speichern\n",
    "with open(json_embedding_path, 'w', encoding='utf-8') as f:\n",
    "\n",
    "    json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Beispiel (mit Seiteninformation): Chunk 0 -> Seite(n): {output_data[0].get('pages', 'N/A')}\")\n",
    "\n",
    "print(f\"Anzahl gespeicherter Embeddings: {len(output_data)}\")\n",
    "print(f\"Embeddings gespeichert in: {json_embedding_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1974902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB importiert\n"
     ]
    }
   ],
   "source": [
    "# ChromaDB importieren\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "print(\"ChromaDB importiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ec23c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB Collection 'geschichtsbuch_collection' erstellt/geladen\n",
      "Anzahl bestehender Dokumente in Collection: 369\n"
     ]
    }
   ],
   "source": [
    "# ChromaDB Client und Collection erstellen\n",
    "# PersistentClient speichert die Daten dauerhaft\n",
    "chroma_client = chromadb.PersistentClient(path=chroma_collection_db)\n",
    "\n",
    "# Collection erstellen oder abrufen\n",
    "# Der Name sollte beschreibend sein\n",
    "collection_name = chroma_collection\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"description\": f\"Embeddings für {extract} mit multilingual-e5-small\"}\n",
    ")\n",
    "\n",
    "print(f\"ChromaDB Collection '{collection_name}' erstellt/geladen\")\n",
    "print(f\"Anzahl bestehender Dokumente in Collection: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "31f7779a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten vorbereitet: 369 Dokumente\n",
      "Beispiel Metadaten: {'chunk_id': 0, 'length': 1500, 'pages': '[12]', 'start_pos': 0, 'end_pos': 1500, 'chunk_index': 0, 'page_numbers': '12'}\n"
     ]
    }
   ],
   "source": [
    "# Daten für ChromaDB vorbereiten\n",
    "ids = []\n",
    "documents = []\n",
    "embeddings_list = []\n",
    "metadatas = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # ID für jedes Dokument (muss eindeutig sein)\n",
    "    doc_id = f\"chunk_{i}\"\n",
    "    ids.append(doc_id)\n",
    "    \n",
    "    # Text extrahieren\n",
    "    if isinstance(chunk, dict):\n",
    "        text = chunk.get('text', chunk.get('content', str(chunk)))\n",
    "        # Optionale Metadaten aus dem Chunk extrahieren\n",
    "        metadata = {k: v for k, v in chunk.items() if k not in ['text', 'content', 'embedding']}\n",
    "        metadata['chunk_index'] = i\n",
    "        \n",
    "        # Explizit Seiteninformationen hinzufügen\n",
    "        if 'pages' in chunk and chunk['pages']:\n",
    "            # Konvertiere Liste zu String für ChromaDB-Kompatibilität\n",
    "            metadata['pages'] = str(chunk['pages'])\n",
    "            metadata['page_numbers'] = ', '.join(map(str, chunk['pages']))\n",
    "        else:\n",
    "            metadata['pages'] = None\n",
    "            metadata['page_numbers'] = 'N/A'\n",
    "    else:\n",
    "        text = chunk\n",
    "        metadata = {\n",
    "            'chunk_index': i,\n",
    "            'pages': None,\n",
    "            'page_numbers': 'N/A'\n",
    "        }\n",
    "    \n",
    "    documents.append(text)\n",
    "    embeddings_list.append(embeddings[i].tolist())\n",
    "    metadatas.append(metadata)\n",
    "\n",
    "print(f\"Daten vorbereitet: {len(ids)} Dokumente\")\n",
    "print(f\"Beispiel Metadaten: {metadatas[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b07226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Collection enthält bereits 369 Dokumente.\n",
      "Lösche bestehende Collection 'geschichtsbuch_collection'...\n",
      "✓ Collection gelöscht.\n",
      "✓ Collection neu erstellt.\n",
      "\n",
      "✓ Embeddings in ChromaDB gespeichert!\n",
      "  Collection: geschichtsbuch_collection\n",
      "  Anzahl Dokumente: 369\n"
     ]
    }
   ],
   "source": [
    "# In ChromaDB speichern\n",
    "# Collection löschen und neu erstellen für frische Daten\n",
    "if collection.count() > 0:\n",
    "    print(f\"⚠️  Collection enthält bereits {collection.count()} Dokumente.\")\n",
    "    print(f\"Lösche bestehende Collection '{collection_name}'...\")\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "    print(\"✓ Collection gelöscht.\")\n",
    "    \n",
    "    # Collection neu erstellen\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"description\": f\"Embeddings für {extract} mit multilingual-e5-small\"}\n",
    "    )\n",
    "    print(\"✓ Collection neu erstellt.\")\n",
    "\n",
    "# Neue Daten hinzufügen (jetzt in leere Collection)\n",
    "collection.add(\n",
    "    ids=ids,\n",
    "    documents=documents,\n",
    "    embeddings=embeddings_list,\n",
    "    metadatas=metadatas\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Embeddings in ChromaDB gespeichert!\")\n",
    "print(f\"  Collection: {collection_name}\")\n",
    "print(f\"  Anzahl Dokumente: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a1e848",
   "metadata": {},
   "source": [
    "## Test: Ähnlichkeitssuche mit ChromaDB\n",
    "\n",
    "Teste die Suche in der ChromaDB mit einer Beispiel-Query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a9fff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'Wer war Wilhelm Tell?'\n",
      "\n",
      "Top 3 ähnlichste Chunks:\n",
      "\n",
      "1. (Distanz: 0.3789)\n",
      "   Seite(n): 59\n",
      "   Chunk Index: 68\n",
      "   Text:  Torwächter und Kriegsknechte und steckten die Burg in Brand . Der Vogt wurde gefangen ge nommen und mußte auf der Landesgrenze den Eid schwören , das Land nie mehr betreten zu wollen. - Zu gleicher Z...\n",
      "\n",
      "2. (Distanz: 0.4116)\n",
      "   Seite(n): 57, 58, 59\n",
      "   Chunk Index: 67\n",
      "   Text:  Apfel vom Kopfe zu schießen. Teil bat , man möge ihm die Strafe erlassen . Geßler aber drohte, erwerbe ihn mit dem Kinde töten lassen . Da betete Tell zu Gott , steckte noch einen zweiten Pfeil in de...\n",
      "\n",
      "3. (Distanz: 0.4183)\n",
      "   Seite(n): 57\n",
      "   Chunk Index: 66\n",
      "   Text: , wie sie das fremde Joch abschütteln können . Stauffacher begab sich alsbald nach Uri zu Walter Fürst, einem edeln , erfahrenen Manne , und traf dort auch den flüchtigen Arnold AnderHalden . Sie besc...\n"
     ]
    }
   ],
   "source": [
    "# Beispiel-Query für Ähnlichkeitssuche\n",
    "query_text = \"Wer war Wilhelm Tell?\"  # Passen Sie die Query an Ihre Daten an\n",
    "\n",
    "# Query-Embedding erstellen (mit \"query:\" Prefix für E5)\n",
    "query_with_prefix = f\"query: {query_text}\"\n",
    "query_encoded = tokenizer(\n",
    "    [query_with_prefix],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    query_outputs = model(**query_encoded)\n",
    "    attention_mask = query_encoded['attention_mask']\n",
    "    token_embeddings = query_outputs.last_hidden_state\n",
    "    \n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    query_embedding = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=1)\n",
    "    query_embedding = query_embedding.cpu().numpy()[0]\n",
    "\n",
    "# Suche in ChromaDB\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding.tolist()],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "print(f\"Query: '{query_text}'\\n\")\n",
    "print(\"Top 3 ähnlichste Chunks:\")\n",
    "for i, (doc, distance, metadata) in enumerate(zip(results['documents'][0], results['distances'][0], results['metadatas'][0])):\n",
    "    print(f\"\\n{i+1}. (Distanz: {distance:.4f})\")\n",
    "    pages_info = metadata.get('page_numbers', 'N/A')\n",
    "    print(f\"   Seite(n): {pages_info}\")\n",
    "    print(f\"   Chunk Index: {metadata.get('chunk_index', 'N/A')}\")\n",
    "    print(f\"   Text: {doc[:200]}...\")  # Nur erste 200 Zeichen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
