{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8dfcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_embedding_path is: docs/embeddings_geschichtsbuch.json\n",
      "chroma_collection is: geschichtsbuch_collection\n",
      "chroma_collection_db is: docs/geschichtsbuch_chroma_db\n"
     ]
    }
   ],
   "source": [
    "# Pfad zu Embedding festlegen\n",
    "\n",
    "#extract = \"origin\"\n",
    "extract = \"geschichtsbuch\"\n",
    "\n",
    "json_embedding_geschichtsbuch = \"docs/embeddings_geschichtsbuch.json\"\n",
    "json_embedding_origin = \"docs/embeddings_dan_brown.json\"\n",
    "\n",
    "chroma_collection_geschichtsbuch = \"geschichtsbuch_collection\"\n",
    "chroma_collection_origin = \"dan_brown_collection\"\n",
    "\n",
    "chroma_db_geschichtsbuch = \"docs/geschichtsbuch_chroma_db\"\n",
    "chroma_db_origin = \"docs/dan_brown_chroma_db\"\n",
    "\n",
    "\n",
    "if extract == \"geschichtsbuch\": \n",
    "    json_embedding_path = json_embedding_geschichtsbuch\n",
    "    chroma_collection = chroma_collection_geschichtsbuch\n",
    "    chroma_collection_db = chroma_db_geschichtsbuch\n",
    "else:\n",
    "    json_embedding_path = json_embedding_origin\n",
    "    chroma_collection = chroma_collection_origin\n",
    "    chroma_collection_db = chroma_db_origin\n",
    "\n",
    "print(f\"json_embedding_path is: {json_embedding_path}\")\n",
    "print(f\"chroma_collection is: {chroma_collection}\")\n",
    "print(f\"chroma_collection_db is: {chroma_collection_db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf23af4a",
   "metadata": {},
   "source": [
    "# LLM Q&A System mit GPT-4o\n",
    "\n",
    "Frage-Antwort-System mit ChromaDB und OpenAI GPT-4o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fd50b4",
   "metadata": {},
   "source": [
    "## Libraries importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aac7a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Using cached openai-2.15.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Using cached jiter-0.12.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from openai) (2.12.5)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Using cached openai-2.15.0-py3-none-any.whl (1.1 MB)\n",
      "Using cached jiter-0.12.0-cp311-cp311-macosx_11_0_arm64.whl (320 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sniffio, jiter, openai\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/3\u001b[0m [openai]2m2/3\u001b[0m [openai]\n",
      "\u001b[1A\u001b[2KSuccessfully installed jiter-0.12.0 openai-2.15.0 sniffio-1.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c229cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries importiert\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "print(\"Libraries importiert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d2e08",
   "metadata": {},
   "source": [
    "## OpenAI API Key laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db787668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì OpenAI API Key geladen\n",
      "  Key beginnt mit: sk-proj-Bk...\n"
     ]
    }
   ],
   "source": [
    "# OpenAI API Key aus Datei laden\n",
    "with open('open_ai_key.txt', 'r') as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "# OpenAI Client initialisieren\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"OpenAI API Key geladen\")\n",
    "print(f\"  Key beginnt mit: {api_key[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bfa677",
   "metadata": {},
   "source": [
    "## Embedding-Modell laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b30927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embedding-Modell geladen: intfloat/multilingual-e5-small\n"
     ]
    }
   ],
   "source": [
    "# Embedding-Modell f√ºr Query-Erstellung laden (gleich wie beim Indexieren)\n",
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Embedding-Modell geladen: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e5694",
   "metadata": {},
   "source": [
    "## ChromaDB Collection laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a973c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ChromaDB Collection geladen: geschichtsbuch_collection\n",
      "  Anzahl Dokumente: 369\n"
     ]
    }
   ],
   "source": [
    "# ChromaDB Client und Collection laden\n",
    "chroma_client = chromadb.PersistentClient(path=chroma_collection_db)\n",
    "collection = chroma_client.get_collection(name=chroma_collection)\n",
    "\n",
    "print(f\"ChromaDB Collection geladen: {chroma_collection}\")\n",
    "print(f\"  Anzahl Dokumente: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de57289",
   "metadata": {},
   "source": [
    "## Hilfsfunktionen definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c180f420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Hilfsfunktionen definiert\n"
     ]
    }
   ],
   "source": [
    "def create_query_embedding(query_text):\n",
    "    \"\"\"\n",
    "    Erstellt ein Embedding f√ºr eine Query-Frage\n",
    "    \"\"\"\n",
    "    query_with_prefix = f\"query: {query_text}\"\n",
    "    query_encoded = tokenizer(\n",
    "        [query_with_prefix],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        query_outputs = model(**query_encoded)\n",
    "        attention_mask = query_encoded['attention_mask']\n",
    "        token_embeddings = query_outputs.last_hidden_state\n",
    "        \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        query_embedding = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=1)\n",
    "        query_embedding = query_embedding.cpu().numpy()[0]\n",
    "    \n",
    "    return query_embedding\n",
    "\n",
    "\n",
    "def search_similar_chunks(query_text, n_results=3):\n",
    "    \"\"\"\n",
    "    Sucht √§hnliche Chunks in ChromaDB basierend auf der Query\n",
    "    \"\"\"\n",
    "    query_embedding = create_query_embedding(query_text)\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def format_context(results):\n",
    "    \"\"\"\n",
    "    Formatiert die gefundenen Chunks als Kontext f√ºr das LLM\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    \n",
    "    for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "        pages = metadata.get('page_numbers', 'N/A')\n",
    "        context_parts.append(f\"[Quelle {i+1}, Seite(n): {pages}]\\n{doc}\")\n",
    "    \n",
    "    return \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "print(\"Hilfsfunktionen definiert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6cd7fb",
   "metadata": {},
   "source": [
    "## RAG-Funktion mit GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79a389ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì RAG-Funktion definiert\n"
     ]
    }
   ],
   "source": [
    "def answer_question(question, n_results=3, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Beantwortet eine Frage mit GPT-4o unter Verwendung von RAG\n",
    "    \n",
    "    Args:\n",
    "        question: Die zu beantwortende Frage\n",
    "        n_results: Anzahl der zu suchenden relevanten Chunks\n",
    "        temperature: Kreativit√§t der Antwort (0.0 - 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        dict mit 'answer', 'sources' und 'context'\n",
    "    \"\"\"\n",
    "    print(f\"Suche relevante Informationen f√ºr: '{question}'\")\n",
    "    \n",
    "    # 1. Relevante Chunks finden\n",
    "    results = search_similar_chunks(question, n_results=n_results)\n",
    "    \n",
    "    # 2. Kontext formatieren\n",
    "    context = format_context(results)\n",
    "    \n",
    "    print(f\"{len(results['documents'][0])} relevante Chunks gefunden\")\n",
    "    print(\"Generiere Antwort mit GPT-4o...\")\n",
    "    \n",
    "    # 3. System-Prompt und User-Prompt erstellen\n",
    "    system_prompt = \"\"\"Du bist ein hilfreicher Assistent, der Fragen basierend auf bereitgestellten Quelltexten beantwortet.\n",
    "\n",
    "Regeln:\n",
    "- Beantworte die Frage nur basierend auf den bereitgestellten Quellen\n",
    "- Zitiere die relevanten Quellen in deiner Antwort (z.B. [Quelle 1])\n",
    "- Wenn die Antwort nicht in den Quellen zu finden ist, sage das ehrlich\n",
    "- Gib pr√§zise und sachliche Antworten\n",
    "- Antworte auf Deutsch\n",
    "- Beschr√§nke dich auf 2 S√§tze\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Kontext aus den Quelltexten:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Bitte beantworte die Frage basierend auf dem obigen Kontext.\"\"\"\n",
    "\n",
    "    # 4. GPT-4o API aufrufen\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    print(\"Antwort generiert!\\n\")\n",
    "    \n",
    "    # 5. Ergebnis zur√ºckgeben\n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'sources': results['metadatas'][0],\n",
    "        'context': context,\n",
    "        'distances': results['distances'][0]\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"RAG-Funktion definiert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec41241b",
   "metadata": {},
   "source": [
    "## Test: Frage stellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0770eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Suche relevante Informationen f√ºr: 'Wer war Wilhelm Tell?'\n",
      "‚úì 3 relevante Chunks gefunden\n",
      "üí≠ Generiere Antwort mit GPT-4o...\n",
      "‚úì Antwort generiert!\n",
      "\n",
      "================================================================================\n",
      "FRAGE: Wer war Wilhelm Tell?\n",
      "================================================================================\n",
      "\n",
      "ANTWORT:\n",
      "Wilhelm Tell war ein trefflicher Bogensch√ºtze aus B√ºrzeln, der bekannt wurde, weil er sich weigerte, einem aufgestellten √∂sterreichischen Hut die geb√ºhrende Ehre zu erweisen. Dies f√ºhrte dazu, dass er vor den Vogt Ge√üler gebracht wurde, der ihm zur Strafe befahl, einen Apfel vom Kopf seines eigenen Kindes zu schie√üen. Tell gelang dies, nachdem er zu Gott gebetet und einen zweiten Pfeil in den K√∂cher gesteckt hatte. Als Ge√üler ihn fragte, was dieser zweite Pfeil zu bedeuten habe, antwortete Tell, dass dieser f√ºr Ge√ülers Herz bestimmt gewesen w√§re, h√§tte der erste Pfeil sein Kind getroffen [Quelle 2, 3].\n",
      "\n",
      "================================================================================\n",
      "QUELLEN:\n",
      "================================================================================\n",
      "\n",
      "1. Seite(n): 59 | √Ñhnlichkeit: 0.621\n",
      "\n",
      "2. Seite(n): 57, 58, 59 | √Ñhnlichkeit: 0.588\n",
      "\n",
      "3. Seite(n): 57 | √Ñhnlichkeit: 0.582\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Beispiel-Frage\n",
    "question = \"Wer war Wilhelm Tell?\"\n",
    "\n",
    "# Frage beantworten\n",
    "result = answer_question(question, n_results=3)\n",
    "\n",
    "# Ausgabe formatieren\n",
    "print(\"=\"*80)\n",
    "print(f\"FRAGE: {question}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nANTWORT:\\n{result['answer']}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUELLEN:\")\n",
    "print(\"=\"*80)\n",
    "for i, source in enumerate(result['sources']):\n",
    "    distance = result['distances'][i]\n",
    "    pages = source.get('page_numbers', 'N/A')\n",
    "    print(f\"\\n{i+1}. Seite(n): {pages} | √Ñhnlichkeit: {1-distance:.3f}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36e8e8",
   "metadata": {},
   "source": [
    "## Interaktive Q&A Schleife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0ab43ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Interaktive Q&A-Funktion definiert\n",
      "  Starten mit: interactive_qa()\n"
     ]
    }
   ],
   "source": [
    "# Interaktive Fragen-Loop\n",
    "# Hinweis: In Jupyter funktioniert input() - zum Beenden 'exit' eingeben\n",
    "\n",
    "def interactive_qa():\n",
    "    \"\"\"\n",
    "    Interaktive Q&A-Schleife\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"Q&A System mit GPT-4o\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Stelle deine Fragen. Beenden mit 'exit', 'quit' oder 'q'\\n\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"Deine Frage: \").strip()\n",
    "        \n",
    "        if question.lower() in ['exit', 'quit', 'q', '']:\n",
    "            print(\"\\nAuf Wiedersehen!\")\n",
    "            break\n",
    "        \n",
    "        print()\n",
    "        result = answer_question(question, n_results=3)\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"ANTWORT:\")\n",
    "        print(\"=\"*80)\n",
    "        print(result['answer'])\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"Quellen:\")\n",
    "        for i, source in enumerate(result['sources']):\n",
    "            pages = source.get('page_numbers', 'N/A')\n",
    "            print(f\"  [{i+1}] Seite(n): {pages}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Starte die interaktive Schleife\n",
    "# interactive_qa()  # Auskommentiert - manuell starten wenn gew√ºnscht\n",
    "\n",
    "print(\"Interaktive Q&A-Funktion definiert\")\n",
    "print(\"  Starten mit: interactive_qa()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c851d8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q&A System mit GPT-4o\n",
      "================================================================================\n",
      "Stelle deine Fragen. Beenden mit 'exit', 'quit' oder 'q'\n",
      "\n",
      "\n",
      "üîç Suche relevante Informationen f√ºr: 'was ist der inhalt des buches'\n",
      "‚úì 3 relevante Chunks gefunden\n",
      "üí≠ Generiere Antwort mit GPT-4o...\n",
      "‚úì Antwort generiert!\n",
      "\n",
      "================================================================================\n",
      "ANTWORT:\n",
      "================================================================================\n",
      "Der Inhalt des Buches scheint sich mit geschichtlichen und verfassungsgeschichtlichen Themen zu befassen. Es behandelt die Darstellung gesetzgeberischer T√§tigkeiten, den Kampf gegen die Aristokratie, die Erwerbung der tessinischen Vogteien sowie verschiedene kultur- und verfassungsgeschichtliche R√ºckblicke. Das Buch ist so konzipiert, dass es den freien Vortrag des Lehrers unterst√ºtzt und den Sch√ºlern erm√∂glicht, die Inhalte zu Hause zu vertiefen. Es enth√§lt ebenfalls Abschnitte √ºber religi√∂se Verh√§ltnisse, Erwerbsleben, Schulwesen, Kunst und Wissenschaft [Quelle 1]. \n",
      "\n",
      "Zus√§tzlich gibt es Berichte √ºber historische Ereignisse und Figuren, etwa die Geschichte von Wilhelm Tell, der gezwungen wird, einen Apfel vom Kopf seines Sohnes zu schie√üen, und die Vertreibung der V√∂gte im Jahr 1308 [Quelle 3].\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Quellen:\n",
      "  [1] Seite(n): 13, 14\n",
      "  [2] Seite(n): 248\n",
      "  [3] Seite(n): 57, 58, 59\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üîç Suche relevante Informationen f√ºr: 'Was ist der Rhein'\n",
      "‚úì 3 relevante Chunks gefunden\n",
      "üí≠ Generiere Antwort mit GPT-4o...\n",
      "‚úì Antwort generiert!\n",
      "\n",
      "================================================================================\n",
      "ANTWORT:\n",
      "================================================================================\n",
      "Der Rhein wird in den bereitgestellten Quellen nicht direkt beschrieben. Die Quellen erw√§hnen den Rhein in verschiedenen historischen Kontexten, z.B. als geografische Grenze oder Gebiet, das unter verschiedene Herrschaften fiel, jedoch gibt es keine spezifische Beschreibung des Rheins selbst. Wenn du mehr Informationen √ºber den Rhein suchst, m√ºsstest du auf andere Quellen au√üerhalb der bereitgestellten Texte zur√ºckgreifen.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Quellen:\n",
      "  [1] Seite(n): 36, 37\n",
      "  [2] Seite(n): 46, 47\n",
      "  [3] Seite(n): 245, 248\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üëã Auf Wiedersehen!\n"
     ]
    }
   ],
   "source": [
    "interactive_qa()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
