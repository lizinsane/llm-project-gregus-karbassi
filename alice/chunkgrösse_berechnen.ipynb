{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6729c2a2",
   "metadata": {},
   "source": [
    "# Chunk-Gr√∂√üe f√ºr multilingual-e5-small bestimmen\n",
    "\n",
    "Das Modell hat eine **maximale Token-L√§nge von 512 Tokens**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edecccf4",
   "metadata": {},
   "source": [
    "## Installation und Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21046f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Lade den Tokenizer f√ºr multilingual-e5-small\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
    "print(\"Tokenizer geladen!\")\n",
    "print(f\"Max. Model-Token-L√§nge: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b1561",
   "metadata": {},
   "source": [
    "## Test mit Beispieltext aus deinem PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste mit unterschiedlich langen Texten\n",
    "test_texts = {\n",
    "    \"100 W√∂rter\": \" \".join([\"Wort\"] * 100),\n",
    "    \"200 W√∂rter\": \" \".join([\"Wort\"] * 200),\n",
    "    \"300 W√∂rter\": \" \".join([\"Wort\"] * 300),\n",
    "    \"400 W√∂rter\": \" \".join([\"Wort\"] * 400),\n",
    "}\n",
    "\n",
    "print(\"Token-Anzahl f√ºr verschiedene Textl√§ngen:\\n\")\n",
    "for name, text in test_texts.items():\n",
    "    # Wichtig: Prefix \"passage: \" hinzuf√ºgen (wird vom Modell erwartet!)\n",
    "    text_with_prefix = f\"passage: {text}\"\n",
    "    tokens = tokenizer(text_with_prefix, return_tensors='pt')\n",
    "    num_tokens = tokens['input_ids'].shape[1]\n",
    "    print(f\"{name}: {num_tokens} Tokens\")\n",
    "    \n",
    "print(f\"\\n‚ö†Ô∏è Maximale Token-L√§nge: 512 Tokens\")\n",
    "print(\"Texte werden automatisch abgeschnitten, wenn sie l√§nger sind!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4403e6a4",
   "metadata": {},
   "source": [
    "## Test mit echtem Text aus deinem Geschichtsbuch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lade einen Teil deines extrahierten Textes\n",
    "with open(\"docs/extracted_content_geschichtsbuch.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "# Teste verschiedene Chunk-Gr√∂√üen (in Zeichen)\n",
    "chunk_sizes = [500, 1000, 1500, 2000, 2500, 3000]\n",
    "\n",
    "print(\"Empfohlene Chunk-Gr√∂√üen (in Zeichen):\\n\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    # Nimm einen Chunk dieser Gr√∂√üe\n",
    "    sample_chunk = full_text[:chunk_size]\n",
    "    \n",
    "    # Tokenisiere mit Prefix\n",
    "    text_with_prefix = f\"passage: {sample_chunk}\"\n",
    "    tokens = tokenizer(text_with_prefix, return_tensors='pt', truncation=False)\n",
    "    num_tokens = tokens['input_ids'].shape[1]\n",
    "    \n",
    "    status = \"‚úÖ OK\" if num_tokens <= 512 else f\"‚ùå ZU LANG ({num_tokens - 512} Tokens zu viel)\"\n",
    "    \n",
    "    print(f\"Chunk: {chunk_size:4d} Zeichen ‚Üí {num_tokens:3d} Tokens | {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° EMPFEHLUNG:\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038cacb9",
   "metadata": {},
   "source": [
    "## Optimale Chunk-Gr√∂√üe berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0338a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finde die optimale Chunk-Gr√∂√üe durch bin√§re Suche\n",
    "def find_optimal_chunk_size(text_sample, max_tokens=510):\n",
    "    \"\"\"\n",
    "    Findet die maximale Chunk-Gr√∂√üe in Zeichen, die nicht √ºber max_tokens kommt.\n",
    "    510 statt 512, um etwas Puffer zu lassen.\n",
    "    \"\"\"\n",
    "    low, high = 100, 5000\n",
    "    best_size = 100\n",
    "    \n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        chunk = text_sample[:mid]\n",
    "        text_with_prefix = f\"passage: {chunk}\"\n",
    "        tokens = tokenizer(text_with_prefix, return_tensors='pt', truncation=False)\n",
    "        num_tokens = tokens['input_ids'].shape[1]\n",
    "        \n",
    "        if num_tokens <= max_tokens:\n",
    "            best_size = mid\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            high = mid - 1\n",
    "    \n",
    "    return best_size\n",
    "\n",
    "# Teste mit verschiedenen Textabschnitten\n",
    "sample1 = full_text[1000:6000]  # Mittelteil\n",
    "sample2 = full_text[10000:15000]  # Anderer Teil\n",
    "\n",
    "optimal_size1 = find_optimal_chunk_size(sample1)\n",
    "optimal_size2 = find_optimal_chunk_size(sample2)\n",
    "\n",
    "optimal_size = min(optimal_size1, optimal_size2)\n",
    "\n",
    "print(f\"Optimale Chunk-Gr√∂√üe (Probe 1): {optimal_size1} Zeichen\")\n",
    "print(f\"Optimale Chunk-Gr√∂√üe (Probe 2): {optimal_size2} Zeichen\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìä EMPFOHLENE CHUNK-GR√ñSSE: {optimal_size} Zeichen\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nDas entspricht ungef√§hr {optimal_size // 5} W√∂rtern (Durchschnitt)\")\n",
    "print(f\"Mit Overlap von 10-20% = {int(optimal_size * 0.1)}-{int(optimal_size * 0.2)} Zeichen\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
