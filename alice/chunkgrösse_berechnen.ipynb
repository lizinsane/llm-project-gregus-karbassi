{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6729c2a2",
   "metadata": {},
   "source": [
    "# Chunk-Gr√∂√üe f√ºr multilingual-e5-small bestimmen\n",
    "\n",
    "Das Modell hat eine **maximale Token-L√§nge von 512 Tokens**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edecccf4",
   "metadata": {},
   "source": [
    "## Installation und Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d5a505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (4.57.6)\n",
      "Requirement already satisfied: torch in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: requests in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2026.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from requests->transformers) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages (from requests->transformers) (2026.1.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21046f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alice/Repositorys/llm-project-gregus-karbassi/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer geladen!\n",
      "Max. Model-Token-L√§nge: 512\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Lade den Tokenizer f√ºr multilingual-e5-small\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
    "print(\"Tokenizer geladen!\")\n",
    "print(f\"Max. Model-Token-L√§nge: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b1561",
   "metadata": {},
   "source": [
    "## Test mit Beispieltext aus deinem PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f70402e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-Anzahl f√ºr verschiedene Textl√§ngen:\n",
      "\n",
      "100 W√∂rter: 104 Tokens\n",
      "200 W√∂rter: 204 Tokens\n",
      "300 W√∂rter: 304 Tokens\n",
      "400 W√∂rter: 404 Tokens\n",
      "\n",
      "‚ö†Ô∏è Maximale Token-L√§nge: 512 Tokens\n",
      "Texte werden automatisch abgeschnitten, wenn sie l√§nger sind!\n"
     ]
    }
   ],
   "source": [
    "# Teste mit unterschiedlich langen Texten\n",
    "test_texts = {\n",
    "    \"100 W√∂rter\": \" \".join([\"Wort\"] * 100),\n",
    "    \"200 W√∂rter\": \" \".join([\"Wort\"] * 200),\n",
    "    \"300 W√∂rter\": \" \".join([\"Wort\"] * 300),\n",
    "    \"400 W√∂rter\": \" \".join([\"Wort\"] * 400),\n",
    "}\n",
    "\n",
    "print(\"Token-Anzahl f√ºr verschiedene Textl√§ngen:\\n\")\n",
    "for name, text in test_texts.items():\n",
    "    # Wichtig: Prefix \"passage: \" hinzuf√ºgen (wird vom Modell erwartet!)\n",
    "    text_with_prefix = f\"passage: {text}\"\n",
    "    tokens = tokenizer(text_with_prefix, return_tensors='pt')\n",
    "    num_tokens = tokens['input_ids'].shape[1]\n",
    "    print(f\"{name}: {num_tokens} Tokens\")\n",
    "    \n",
    "print(f\"\\n‚ö†Ô∏è Maximale Token-L√§nge: 512 Tokens\")\n",
    "print(\"Texte werden automatisch abgeschnitten, wenn sie l√§nger sind!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4403e6a4",
   "metadata": {},
   "source": [
    "## Test mit echtem Text aus deinem Geschichtsbuch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d207737a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empfohlene Chunk-Gr√∂√üen (in Zeichen):\n",
      "\n",
      "------------------------------------------------------------\n",
      "Chunk:  500 Zeichen ‚Üí 123 Tokens | ‚úÖ OK\n",
      "Chunk: 1000 Zeichen ‚Üí 241 Tokens | ‚úÖ OK\n",
      "Chunk: 1500 Zeichen ‚Üí 339 Tokens | ‚úÖ OK\n",
      "Chunk: 2000 Zeichen ‚Üí 445 Tokens | ‚úÖ OK\n",
      "Chunk: 2500 Zeichen ‚Üí 651 Tokens | ‚ùå ZU LANG (139 Tokens zu viel)\n",
      "Chunk: 3000 Zeichen ‚Üí 787 Tokens | ‚ùå ZU LANG (275 Tokens zu viel)\n",
      "\n",
      "============================================================\n",
      "üí° EMPFEHLUNG:\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Lade einen Teil deines extrahierten Textes\n",
    "with open(\"docs/extracted_content_geschichtsbuch.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "# Teste verschiedene Chunk-Gr√∂√üen (in Zeichen)\n",
    "chunk_sizes = [500, 1000, 1500, 2000, 2500, 3000]\n",
    "\n",
    "print(\"Empfohlene Chunk-Gr√∂√üen (in Zeichen):\\n\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    # Nimm einen Chunk dieser Gr√∂√üe\n",
    "    sample_chunk = full_text[:chunk_size]\n",
    "    \n",
    "    # Tokenisiere mit Prefix\n",
    "    text_with_prefix = f\"passage: {sample_chunk}\"\n",
    "    tokens = tokenizer(text_with_prefix, return_tensors='pt', truncation=False)\n",
    "    num_tokens = tokens['input_ids'].shape[1]\n",
    "    \n",
    "    status = \"‚úÖ OK\" if num_tokens <= 512 else f\"‚ùå ZU LANG ({num_tokens - 512} Tokens zu viel)\"\n",
    "    \n",
    "    print(f\"Chunk: {chunk_size:4d} Zeichen ‚Üí {num_tokens:3d} Tokens | {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° EMPFEHLUNG:\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038cacb9",
   "metadata": {},
   "source": [
    "## Optimale Chunk-Gr√∂√üe berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c0338a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimale Chunk-Gr√∂√üe (Probe 1): 1802 Zeichen\n",
      "Optimale Chunk-Gr√∂√üe (Probe 2): 2025 Zeichen\n",
      "\n",
      "============================================================\n",
      "üìä EMPFOHLENE CHUNK-GR√ñSSE: 1802 Zeichen\n",
      "============================================================\n",
      "\n",
      "Das entspricht ungef√§hr 360 W√∂rtern (Durchschnitt)\n",
      "Mit Overlap von 10-20% = 180-360 Zeichen\n"
     ]
    }
   ],
   "source": [
    "# Finde die optimale Chunk-Gr√∂√üe durch bin√§re Suche\n",
    "def find_optimal_chunk_size(text_sample, max_tokens=510):\n",
    "    \"\"\"\n",
    "    Findet die maximale Chunk-Gr√∂√üe in Zeichen, die nicht √ºber max_tokens kommt.\n",
    "    510 statt 512, um etwas Puffer zu lassen.\n",
    "    \"\"\"\n",
    "    low, high = 100, 5000\n",
    "    best_size = 100\n",
    "    \n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        chunk = text_sample[:mid]\n",
    "        text_with_prefix = f\"passage: {chunk}\"\n",
    "        tokens = tokenizer(text_with_prefix, return_tensors='pt', truncation=False)\n",
    "        num_tokens = tokens['input_ids'].shape[1]\n",
    "        \n",
    "        if num_tokens <= max_tokens:\n",
    "            best_size = mid\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            high = mid - 1\n",
    "    \n",
    "    return best_size\n",
    "\n",
    "# Teste mit verschiedenen Textabschnitten\n",
    "sample1 = full_text[1000:6000]  # Mittelteil\n",
    "sample2 = full_text[10000:15000]  # Anderer Teil\n",
    "\n",
    "optimal_size1 = find_optimal_chunk_size(sample1)\n",
    "optimal_size2 = find_optimal_chunk_size(sample2)\n",
    "\n",
    "optimal_size = min(optimal_size1, optimal_size2)\n",
    "\n",
    "print(f\"Optimale Chunk-Gr√∂√üe (Probe 1): {optimal_size1} Zeichen\")\n",
    "print(f\"Optimale Chunk-Gr√∂√üe (Probe 2): {optimal_size2} Zeichen\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìä EMPFOHLENE CHUNK-GR√ñSSE: {optimal_size} Zeichen\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nDas entspricht ungef√§hr {optimal_size // 5} W√∂rtern (Durchschnitt)\")\n",
    "print(f\"Mit Overlap von 10-20% = {int(optimal_size * 0.1)}-{int(optimal_size * 0.2)} Zeichen\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
