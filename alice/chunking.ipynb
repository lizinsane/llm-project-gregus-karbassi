{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bad2eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "markdown_path is: docs/extracted_content_geschichtsbuch.md\n",
      "json_chunk_path is: docs/chunks_geschichtsbuch.json\n"
     ]
    }
   ],
   "source": [
    "# Pfad zu Markdown File festlegen\n",
    "\n",
    "#extract = \"origin\"\n",
    "extract = \"geschichtsbuch\"\n",
    "\n",
    "markdown_path_origin = \"docs/extracted_content_dan_brown.md\"\n",
    "markdown_path_path_geschichtsbuch = \"docs/extracted_content_geschichtsbuch.md\" \n",
    "\n",
    "json_chunk_origin = \"docs/chunks_dan_brown.json\"\n",
    "json_chunk_geschichtsbuch = \"docs/chunks_geschichtsbuch.json\"\n",
    "\n",
    "if extract == \"geschichtsbuch\": \n",
    "    markdown_path = markdown_path_path_geschichtsbuch\n",
    "    json_chunk_path = json_chunk_geschichtsbuch\n",
    "else:\n",
    "    markdown_path = markdown_path_origin\n",
    "    json_chunk_path = json_chunk_origin\n",
    "\n",
    "print(f\"markdown_path is: {markdown_path}\")\n",
    "print(f\"json_chunk_path is: {json_chunk_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca428a1b",
   "metadata": {},
   "source": [
    "## Parameter für Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1620af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk-Größe: 1500 Zeichen\n",
      "Overlap: 200 Zeichen\n"
     ]
    }
   ],
   "source": [
    "# Chunking-Parameter\n",
    "CHUNK_SIZE = 1500\n",
    "OVERLAP = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7370b8da",
   "metadata": {},
   "source": [
    "## Markdown-Datei einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "259bb40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datei geladen: docs/extracted_content_geschichtsbuch.md\n",
      "Gesamtlänge: 466497 Zeichen\n",
      "Gesamtlänge: 73166 Wörter\n"
     ]
    }
   ],
   "source": [
    "# Markdown-Datei einlesen\n",
    "with open(markdown_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    markdown_text = f.read()\n",
    "\n",
    "print(f\"Datei geladen: {markdown_path}\")\n",
    "print(f\"Gesamtlänge: {len(markdown_text)} Zeichen\")\n",
    "print(f\"Gesamtlänge: {len(markdown_text.split())} Wörter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f839d9",
   "metadata": {},
   "source": [
    "## Character Splitting implementieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9bbbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Text wurde in 359 Chunks aufgeteilt\n",
      "============================================================\n",
      "\n",
      "Erste 3 Chunks (Vorschau):\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Chunk 1 (Länge: 1500 Zeichen) ---\n",
      "## www.e-rara.ch\n",
      "\n",
      "Die Plattform e-rara.ch macht die in Schweizer Bibliotheken vorhandenen Drucke online verfügbar. Das Spektrum reicht von Büchern über Karten bis zu illustrierten Materialien - von de...\n",
      "\n",
      "--- Chunk 2 (Länge: 1500 Zeichen) ---\n",
      "n the title information for each document individually. For further information please refer to the terms of use on [Link]\n",
      "\n",
      "Conditions d'utilisation Ce document numérique peut être téléchargé gratuite...\n",
      "\n",
      "--- Chunk 3 (Länge: 1500 Zeichen) ---\n",
      "Sutrrs 'Schweizer Gelchichte \" bearbeitet van\n",
      "\n",
      "## I. LrvLler,\n",
      "\n",
      "Lrhrrr an ürr untern Kealschulr in Lufrrn.\n",
      "\n",
      "Mit I &gt; 5 Textbildern , farbiger Wappentafel und 8 farbigen Geschichtskarten der Schweiz n...\n"
     ]
    }
   ],
   "source": [
    "def character_split(text, chunk_size, overlap):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    \n",
    "    while start < text_length:\n",
    "        \n",
    "        end = start + chunk_size # Ende des Chunks\n",
    "        \n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk) # Chunk extrahieren\n",
    "        \n",
    "        start = end - overlap # Nächste Startposition: Ende minus Overlap\n",
    "        \n",
    "        # Verhindere Endlosschleife bei sehr kleinen Texten\n",
    "        if start + chunk_size >= text_length and start < text_length:\n",
    "            # Letzter Chunk\n",
    "            chunks.append(text[start:])\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Text in Chunks aufteilen\n",
    "chunks = character_split(markdown_text, CHUNK_SIZE, OVERLAP)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Text wurde in {len(chunks)} Chunks aufgeteilt\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nErste 3 Chunks (Vorschau):\")\n",
    "print(f\"-\"*60)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} (Länge: {len(chunk)} Zeichen) ---\")\n",
    "    print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cacd88",
   "metadata": {},
   "source": [
    "## Statistiken der Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d53ea19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Chunks: 359\n",
      "Durchschnittliche Chunk-Länge: 1499 Zeichen\n",
      "Kürzester Chunk: 1097 Zeichen\n",
      "Längster Chunk: 1500 Zeichen\n",
      "\n",
      "============================================================\n",
      "Token-Check für ersten Chunk:\n",
      "============================================================\n",
      "Zeichen: 1500\n",
      "Tokens: 339\n",
      "Status: ✅ OK (unter 512)\n"
     ]
    }
   ],
   "source": [
    "# Statistiken über die Chunks\n",
    "chunk_lengths = [len(chunk) for chunk in chunks]\n",
    "\n",
    "print(f\"Anzahl Chunks: {len(chunks)}\")\n",
    "print(f\"Durchschnittliche Chunk-Länge: {sum(chunk_lengths) / len(chunk_lengths):.0f} Zeichen\")\n",
    "print(f\"Kürzester Chunk: {min(chunk_lengths)} Zeichen\")\n",
    "print(f\"Längster Chunk: {max(chunk_lengths)} Zeichen\")\n",
    "\n",
    "# Überprüfe mit Tokenizer (falls geladen)\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
    "    \n",
    "    # Teste ersten Chunk mit Prefix\n",
    "    sample_chunk = f\"passage: {chunks[0]}\"\n",
    "    tokens = tokenizer(sample_chunk, return_tensors='pt', truncation=False)\n",
    "    num_tokens = tokens['input_ids'].shape[1]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Token-Check für ersten Chunk:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Zeichen: {len(chunks[0])}\")\n",
    "    print(f\"Tokens: {num_tokens}\")\n",
    "    print(f\"Status: {'✅ OK (unter 512)' if num_tokens <= 512 else '❌ ZU LANG'}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nTokenizer nicht verfügbar: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b4f73",
   "metadata": {},
   "source": [
    "## Chunks speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d06d48e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 359 Chunks gespeichert in: docs/chunks_geschichtsbuch.json\n",
      "Dateigröße: 565.0 KB\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Dateiname basierend auf dem Quell-Dokument\n",
    "output_file = json_chunk_path\n",
    "\n",
    "# Chunks mit Metadaten speichern\n",
    "chunks_data = {\n",
    "    \"source_file\": markdown_path,\n",
    "    \"chunk_size\": CHUNK_SIZE,\n",
    "    \"overlap\": OVERLAP,\n",
    "    \"total_chunks\": len(chunks),\n",
    "    \"chunks\": [\n",
    "        {\n",
    "            \"chunk_id\": i,\n",
    "            \"text\": chunk,\n",
    "            \"length\": len(chunk)\n",
    "        }\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Als JSON speichern\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ {len(chunks)} Chunks gespeichert in: {output_file}\")\n",
    "print(f\"Dateigröße: {Path(output_file).stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f504f31",
   "metadata": {},
   "source": [
    "## Chunks wieder einlesen (zum Testen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddeffbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geladene Daten:\n",
      "  Quelle: docs/extracted_content_geschichtsbuch.md\n",
      "  Chunk-Größe: 1500\n",
      "  Overlap: 200\n",
      "  Anzahl Chunks: 359\n",
      "\n",
      "✅ 359 Chunks erfolgreich geladen\n",
      "\n",
      "Beispiel - Erster Chunk (erste 150 Zeichen):\n",
      "## www.e-rara.ch\n",
      "\n",
      "Die Plattform e-rara.ch macht die in Schweizer Bibliotheken vorhandenen Drucke online verfügbar. Das Spektrum reicht von Büchern übe...\n"
     ]
    }
   ],
   "source": [
    "# Chunks wieder einlesen\n",
    "with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded_data = json.load(f)\n",
    "\n",
    "# Metadaten anzeigen\n",
    "print(\"Geladene Daten:\")\n",
    "print(f\"  Quelle: {loaded_data['source_file']}\")\n",
    "print(f\"  Chunk-Größe: {loaded_data['chunk_size']}\")\n",
    "print(f\"  Overlap: {loaded_data['overlap']}\")\n",
    "print(f\"  Anzahl Chunks: {loaded_data['total_chunks']}\")\n",
    "\n",
    "# Die eigentlichen Text-Chunks extrahieren\n",
    "loaded_chunks = [chunk_data[\"text\"] for chunk_data in loaded_data[\"chunks\"]]\n",
    "\n",
    "print(f\"\\n✅ {len(loaded_chunks)} Chunks erfolgreich geladen\")\n",
    "print(f\"\\nBeispiel - Erster Chunk (erste 150 Zeichen):\")\n",
    "print(loaded_chunks[0][:150] + \"...\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
